{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mjuCgUcrj0VI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import urllib.request\n",
        "import time\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "authors=[]\n",
        "dates=[]\n",
        "sources=[]\n",
        "statements=[]\n",
        "targets=[]"
      ],
      "metadata": {
        "id": "kOWi2Swmj8q4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_website(page_number):\n",
        "  page_num=str(page_number)\n",
        "\n",
        "  url=\"https://www.politifact.com/factchecks/list/?page={}\".format(page_num)\n",
        "\n",
        "\n",
        "  webpage=requests.get(url)\n",
        "  # time.sleep(3)\n",
        "  soup=BeautifulSoup(webpage.text,'html.parser')\n",
        "\n",
        "  statement_footer = soup.find_all('footer',attrs = {'class': 'm-statement__footer'})\n",
        "  statement_quote = soup.find_all('div',attrs = {'class':'m-statement__quote' })\n",
        "  statement_meta = soup.find_all('div',attrs = {'class':'m-statement__meta'})\n",
        "  target = soup.find_all('div',attrs= {'class':\"m-statement__meter\"})\n",
        "\n",
        "  for i in statement_footer:\n",
        "    link1=i.text.strip()\n",
        "    name_and_date=link1.split()\n",
        "    first_name=name_and_date[1]\n",
        "    last_name=name_and_date[2]\n",
        "    full_name=first_name+' '+last_name\n",
        "    month=name_and_date[4]\n",
        "    day=name_and_date[5]\n",
        "    year=name_and_date[6]\n",
        "    date=month+''+day+''+year\n",
        "    dates.append(date)\n",
        "    authors.append(full_name)\n",
        "\n",
        "  for i in statement_quote:\n",
        "    link2=i.find_all('a')\n",
        "    statements.append(link2[0].text.strip())\n",
        "\n",
        "  for i in statement_meta:\n",
        "    link3=i.find_all('a')\n",
        "    source_text=link3[0].text.strip()\n",
        "    sources.append(source_text)\n",
        "\n",
        "  for i in target:\n",
        "    fact=i.find('div',attrs={'class':'c-image'}).find('img').get('alt')\n",
        "    targets.append(fact)"
      ],
      "metadata": {
        "id": "kgOToVGUkBtX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=8\n",
        "for i in range(1,n):\n",
        "  scrape_website(i)\n",
        "data=pd.DataFrame(columns=['authors','dates','statements','sources','target'])\n",
        "data['author']=authors\n",
        "data['sources']=sources\n",
        "data['date']=dates\n",
        "#data['statement']=statements\n",
        "# data['target']=targets\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "vZdi-inTpfug",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "ed7a5711-01b0-427f-961c-30577642b388"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (3270) does not match length of index (3300)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-fde874afd675>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'authors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dates'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'statements'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sources'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sources'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#data['statement']=statements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-> 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5266\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5267\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (3270) does not match length of index (3300)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import urllib.request\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "authors=[]\n",
        "dates=[]\n",
        "sources=[]\n",
        "statements=[]\n",
        "targets=[]\n",
        "\n",
        "def scrape_website(page_number):\n",
        "  page_num=str(page_number)\n",
        "\n",
        "  url=\"https://www.politifact.com/factchecks/list/?page={}\".format(page_num)\n",
        "\n",
        "\n",
        "  webpage=requests.get(url)\n",
        "  time.sleep(3)\n",
        "  soup=BeautifulSoup(webpage.text,'html.parser')\n",
        "\n",
        "  statement_footer = soup.find_all('footer',attrs = {'class': 'm-statement__footer'})\n",
        "  statement_quote = soup.find_all('div',attrs = {'class':'m-statement__quote' })\n",
        "  statement_meta = soup.find_all('div',attrs = {'class':'m-statement__meta'})\n",
        "  target = soup.find_all('div',attrs= {'class':\"m-statement__meter\"})\n",
        "\n",
        "  for i in statement_footer:\n",
        "    link1=i.text.strip()\n",
        "    name_and_date=link1.split()\n",
        "    first_name=name_and_date[1]\n",
        "    last_name=name_and_date[2]\n",
        "    full_name=first_name+' '+last_name\n",
        "    month=name_and_date[4]\n",
        "    day=name_and_date[5]\n",
        "    year=name_and_date[6]\n",
        "    date=month+' '+day+' '+year\n",
        "    dates.append(date)\n",
        "    authors.append(full_name)\n",
        "\n",
        "  for i in statement_quote:\n",
        "    link2=i.find_all('a')\n",
        "    statements.append(link2[0].text.strip())\n",
        "\n",
        "  for i in statement_meta:\n",
        "    link3=i.find_all('a')\n",
        "    source_text=link3[0].text.strip()\n",
        "    sources.append(source_text)\n",
        "\n",
        "  for i in target:\n",
        "    fact=i.find('div',attrs={'class':'c-image'}).find('img').get('alt')\n",
        "    targets.append(fact)\n",
        "\n",
        "n=100\n",
        "for i in range(1,n):\n",
        "  scrape_website(i)\n",
        "\n",
        "\n",
        "data=pd.DataFrame(columns=['authors','dates','statements','sources','target'])\n",
        "data['author']=authors\n",
        "data['sources']=sources\n",
        "data['date']=dates\n",
        "data['statement']=statements\n",
        "data['target']=targets\n",
        "\n",
        "print(data)\n",
        "data.to_csv('output.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT21xRzK3xem",
        "outputId": "35674364-788e-4435-80a6-fa11bf188b9a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     authors dates statements             sources       target  \\\n",
            "0        NaN   NaN        NaN        TikTok posts        false   \n",
            "1        NaN   NaN        NaN            JD Vance  barely-true   \n",
            "2        NaN   NaN        NaN         Viral image        false   \n",
            "3        NaN   NaN        NaN      Facebook posts        false   \n",
            "4        NaN   NaN        NaN         Viral image        false   \n",
            "...      ...   ...        ...                 ...          ...   \n",
            "2965     NaN   NaN        NaN      Facebook posts   pants-fire   \n",
            "2966     NaN   NaN        NaN      Facebook posts        false   \n",
            "2967     NaN   NaN        NaN  Janet Protasiewicz    half-true   \n",
            "2968     NaN   NaN        NaN        Donald Trump        false   \n",
            "2969     NaN   NaN        NaN             X posts        false   \n",
            "\n",
            "                author              date  \\\n",
            "0           Sara Swann  January 17, 2025   \n",
            "1     Caleb McCullough  January 17, 2025   \n",
            "2       Ciara O'Rourke  January 17, 2025   \n",
            "3       Ciara O'Rourke  January 17, 2025   \n",
            "4       Ciara O'Rourke  January 17, 2025   \n",
            "...                ...               ...   \n",
            "2965      Jeff Cercone    March 27, 2023   \n",
            "2966    Madison Czopek    March 27, 2023   \n",
            "2967     Laura Schulte    March 24, 2023   \n",
            "2968     Maria Ramirez       • March 24,   \n",
            "2969   Loreben Tuquero    March 24, 2023   \n",
            "\n",
            "                                              statement  \n",
            "0     President-elect Donald Trump posted on X that ...  \n",
            "1     “Some of these (California) reservoirs have be...  \n",
            "2     “All three California wild fires seem to start...  \n",
            "3     “Jimmy Kimmel just announced that his show wil...  \n",
            "4     Video shows Mexican firefighters going to help...  \n",
            "...                                                 ...  \n",
            "2965  Photo shows Pennsylvania Sen. John Fetterman h...  \n",
            "2966  “At a top robotics company in Japan this week,...  \n",
            "2967  As a sitting Supreme Court justice, Daniel Kel...  \n",
            "2968  Violent crime in Manhattan is \"now at a record...  \n",
            "2969  “No one got (a) dime” from the $70 million rai...  \n",
            "\n",
            "[2970 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import urllib.request\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "authors=[]\n",
        "dates=[]\n",
        "sources=[]\n",
        "statements=[]\n",
        "targets=[]\n",
        "\n",
        "def scrape_website(page_number):\n",
        "  page_num=str(page_number)\n",
        "\n",
        "  url=\"https://www.politifact.com/factchecks/list/?page={}\".format(page_num)\n",
        "\n",
        "\n",
        "  webpage=requests.get(url)\n",
        "  time.sleep(3)\n",
        "  soup=BeautifulSoup(webpage.text,'html.parser')\n",
        "\n",
        "  statement_footer = soup.find_all('footer',attrs = {'class': 'm-statement__footer'})\n",
        "  statement_quote = soup.find_all('div',attrs = {'class':'m-statement__quote' })\n",
        "  statement_meta = soup.find_all('div',attrs = {'class':'m-statement__meta'})\n",
        "  target = soup.find_all('div',attrs= {'class':\"m-statement__meter\"})\n",
        "\n",
        "  for i in statement_footer:\n",
        "    link1=i.text.strip()\n",
        "    name_and_date=link1.split()\n",
        "    first_name=name_and_date[1]\n",
        "    last_name=name_and_date[2]\n",
        "    full_name=first_name+' '+last_name\n",
        "    month=name_and_date[4]\n",
        "    day=name_and_date[5]\n",
        "    year=name_and_date[6]\n",
        "    date=month+' '+day+' '+year\n",
        "    dates.append(date)\n",
        "    authors.append(full_name)\n",
        "\n",
        "  for i in statement_quote:\n",
        "    link2=i.find_all('a')\n",
        "    statements.append(link2[0].text.strip())\n",
        "\n",
        "  for i in statement_meta:\n",
        "    link3=i.find_all('a')\n",
        "    source_text=link3[0].text.strip()\n",
        "    sources.append(source_text)\n",
        "\n",
        "  for i in target:\n",
        "    fact=i.find('div',attrs={'class':'c-image'}).find('img').get('alt')\n",
        "    targets.append(fact)\n",
        "\n",
        "n=200\n",
        "for i in range(1,n):\n",
        "  scrape_website(i)\n",
        "\n",
        "\n",
        "data=pd.DataFrame(columns=['authors','dates','statements','sources','target'])\n",
        "data['author']=authors\n",
        "data['sources']=sources\n",
        "data['date']=dates\n",
        "data['statement']=statements\n",
        "data['target']=targets\n",
        "\n",
        "print(data)\n",
        "data.to_csv('output1.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTEto19YGigF",
        "outputId": "d84ad7db-da59-4dfd-f317-e74db2f55815"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     authors dates statements              sources       target  \\\n",
            "0        NaN   NaN        NaN         TikTok posts        false   \n",
            "1        NaN   NaN        NaN             JD Vance  barely-true   \n",
            "2        NaN   NaN        NaN          Viral image        false   \n",
            "3        NaN   NaN        NaN       Facebook posts        false   \n",
            "4        NaN   NaN        NaN          Viral image        false   \n",
            "...      ...   ...        ...                  ...          ...   \n",
            "5965     NaN   NaN        NaN       Facebook posts    half-true   \n",
            "5966     NaN   NaN        NaN  Winsome Earle-Sears        false   \n",
            "5967     NaN   NaN        NaN            Tim Kaine         true   \n",
            "5968     NaN   NaN        NaN       Facebook posts   pants-fire   \n",
            "5969     NaN   NaN        NaN       Facebook posts        false   \n",
            "\n",
            "                author              date  \\\n",
            "0           Sara Swann  January 17, 2025   \n",
            "1     Caleb McCullough  January 17, 2025   \n",
            "2       Ciara O'Rourke  January 17, 2025   \n",
            "3       Ciara O'Rourke  January 17, 2025   \n",
            "4       Ciara O'Rourke  January 17, 2025   \n",
            "...                ...               ...   \n",
            "5965     Tom Kertscher      May 27, 2021   \n",
            "5966      Warren Fiske      May 27, 2021   \n",
            "5967     Jon Greenberg      May 27, 2021   \n",
            "5968       Amy Sherman      May 27, 2021   \n",
            "5969     Tom Kertscher      May 26, 2021   \n",
            "\n",
            "                                              statement  \n",
            "0     President-elect Donald Trump posted on X that ...  \n",
            "1     “Some of these (California) reservoirs have be...  \n",
            "2     “All three California wild fires seem to start...  \n",
            "3     “Jimmy Kimmel just announced that his show wil...  \n",
            "4     Video shows Mexican firefighters going to help...  \n",
            "...                                                 ...  \n",
            "5965  Lumber, gas, wheat, coal and corn cost more in...  \n",
            "5966  “The very first Republican convention after th...  \n",
            "5967  “New Haven, Conn., had to issue bonds for a br...  \n",
            "5968         “Boom! Georgia 1st to decertify election.”  \n",
            "5969  Says a photo shows a “lithium mine for hybrid ...  \n",
            "\n",
            "[5970 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import urllib.request\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "authors=[]\n",
        "dates=[]\n",
        "sources=[]\n",
        "statements=[]\n",
        "targets=[]\n",
        "\n",
        "def scrape_website(page_number):\n",
        "  page_num=str(page_number)\n",
        "\n",
        "  url=\"https://www.politifact.com/factchecks/list/?page={}\".format(page_num)\n",
        "\n",
        "\n",
        "  webpage=requests.get(url)\n",
        "  time.sleep(3)\n",
        "  soup=BeautifulSoup(webpage.text,'html.parser')\n",
        "\n",
        "  statement_footer = soup.find_all('footer',attrs = {'class': 'm-statement__footer'})\n",
        "  statement_quote = soup.find_all('div',attrs = {'class':'m-statement__quote' })\n",
        "  statement_meta = soup.find_all('div',attrs = {'class':'m-statement__meta'})\n",
        "  target = soup.find_all('div',attrs= {'class':\"m-statement__meter\"})\n",
        "\n",
        "  for i in statement_footer:\n",
        "    link1=i.text.strip()\n",
        "    name_and_date=link1.split()\n",
        "    first_name=name_and_date[1]\n",
        "    last_name=name_and_date[2]\n",
        "    full_name=first_name+' '+last_name\n",
        "    month=name_and_date[4]\n",
        "    day=name_and_date[5]\n",
        "    year=name_and_date[6]\n",
        "    date=month+' '+day+' '+year\n",
        "    dates.append(date)\n",
        "    authors.append(full_name)\n",
        "\n",
        "  for i in statement_quote:\n",
        "    link2=i.find_all('a')\n",
        "    statements.append(link2[0].text.strip())\n",
        "\n",
        "  for i in statement_meta:\n",
        "    link3=i.find_all('a')\n",
        "    source_text=link3[0].text.strip()\n",
        "    sources.append(source_text)\n",
        "\n",
        "  for i in target:\n",
        "    fact=i.find('div',attrs={'class':'c-image'}).find('img').get('alt')\n",
        "    targets.append(fact)\n",
        "\n",
        "n=300\n",
        "for i in range(1,n):\n",
        "  scrape_website(i)\n",
        "\n",
        "\n",
        "data=pd.DataFrame(columns=['authors','dates','statements','sources','target'])\n",
        "data['author']=authors\n",
        "data['sources']=sources\n",
        "data['date']=dates\n",
        "data['statement']=statements\n",
        "data['target']=targets\n",
        "\n",
        "print(data)\n",
        "data.to_csv('output2.csv',index=False)"
      ],
      "metadata": {
        "id": "lY4ZxpvOKohl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}